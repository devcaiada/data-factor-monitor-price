# ğŸ“Š Criando um Monitoramento de Custos no Data Factory

## ğŸš€ VisÃ£o Geral

Este projeto apresenta uma abordagem prÃ¡tica para explorar o ambiente **Azure** utilizando uma conta gratuita de estudante. O objetivo principal Ã© **configurar o Azure Data Factory e monitorar o uso e os custos dos recursos implantados**. Durante a implementaÃ§Ã£o, abordaremos os seguintes temas:

- âœ”ï¸ EstruturaÃ§Ã£o de assinaturas e grupos de recursos.
- âœ”ï¸ Boas prÃ¡ticas de nomenclatura para facilitar a organizaÃ§Ã£o.
- âœ”ï¸ PersonalizaÃ§Ã£o de dashboards para acompanhamento visual.
- âœ”ï¸ UtilizaÃ§Ã£o de mÃ©tricas e alertas de custo para controle eficiente.
- âœ”ï¸ AutomaÃ§Ã£o com **ARM Templates** e **Azure Cloud Shell**.
- âœ”ï¸ Passo a passo completo para monitorar os gastos na plataforma.
- âœ”ï¸ ImplementaÃ§Ã£o de um **script Python** para automaÃ§Ã£o do monitoramento de custos.

---

## ğŸ› ï¸ Tecnologias Utilizadas

- **Azure Data Factory** ğŸŒ
- **Azure Monitor** ğŸ“Š
- **Azure Cost Management** ğŸ’°
- **Azure Cloud Shell** ğŸ–¥ï¸
- **ARM Templates** ğŸ“„
- **Python + Azure SDK** ğŸ

---

## ğŸ“Œ PrÃ©-requisitos

Antes de iniciar, certifique-se de ter:

- ğŸ”¹ Uma conta gratuita no **Azure for Students** ou **Azure Free Tier**.
- ğŸ”¹ Acesso ao **Azure Portal**.
- ğŸ”¹ Familiaridade com o **Azure Resource Manager (ARM)**.
- ğŸ”¹ Conhecimentos bÃ¡sicos em **PowerShell**, **CLI do Azure** e **Python**.

---

## ğŸ— Passo a Passo da ImplementaÃ§Ã£o

### 1ï¸âƒ£ Criando o Ambiente no Azure

1. Acesse o portal [Azure Portal](https://portal.azure.com/).
2. Crie um **Grupo de Recursos** para organizar os ativos.
3. Configure uma **Assinatura** vinculada Ã  conta gratuita.
4. Defina um **Plano de Nomenclatura** para manter a padronizaÃ§Ã£o.

### 2ï¸âƒ£ Implantando o Azure Data Factory

1. No **Azure Portal**, vÃ¡ atÃ© "Criar um recurso" e selecione **Data Factory**.
2. Escolha o grupo de recursos criado anteriormente.
3. Defina a regiÃ£o e nome do serviÃ§o seguindo as boas prÃ¡ticas.
4. Conclua a implantaÃ§Ã£o e acesse o painel do **Data Factory**.

### 3ï¸âƒ£ Monitorando os Custos

1. Acesse **Azure Cost Management + Billing** no portal.
2. Configure **mÃ©tricas de uso e custo** para o Data Factory.
3. Defina **alertas de custo** para evitar gastos inesperados.
4. Crie um **dashboard personalizado** para visualizaÃ§Ã£o simplificada.

### 4ï¸âƒ£ AutomaÃ§Ã£o com Azure Cloud Shell

1. Acesse o **Azure Cloud Shell** no portal.
2. Utilize **PowerShell** ou **Azure CLI** para gerenciar recursos.
3. Automatize tarefas recorrentes com **scripts ARM Templates**.
4. Teste a criaÃ§Ã£o e remoÃ§Ã£o automatizada de recursos para otimizaÃ§Ã£o.

### 5ï¸âƒ£ ImplementaÃ§Ã£o com Python ğŸš€

Podemos automatizar o monitoramento dos custos do **Azure Data Factory** utilizando Python e a **Azure SDK**. Siga os passos abaixo:

#### ğŸ“Œ Instale as bibliotecas necessÃ¡rias:
```sh
pip install azure-identity azure-mgmt-costmanagement
```

#### ğŸ“ CÃ³digo em Python para Monitoramento de Custos:
```python
from azure.identity import DefaultAzureCredential
from azure.mgmt.costmanagement import CostManagementClient
import datetime


SUBSCRIPTION_ID = "xxxx-xxxx-xxxx-xxxx"
BUDGET_LIMIT = 50


credential = DefaultAzureCredential()
client = CostManagementClient(credential)

# Define o perÃ­odo para consulta (Ãºltimos 30 dias)
end_date = datetime.date.today()
start_date = end_date - datetime.timedelta(days=30)

# Consulta os custos do Data Factory
cost_request = {
    "type": "Usage",
    "timeframe": "Custom",
    "time_period": {
        "from": start_date.strftime('%Y-%m-%d'),
        "to": end_date.strftime('%Y-%m-%d')
    },
    "dataset": {
        "granularity": "Daily",
        "aggregation": {
            "totalCost": {
                "name": "PreTaxCost",
                "function": "Sum"
            }
        }
    }
}


response = client.query.usage(f"/subscriptions/{SUBSCRIPTION_ID}", cost_request)
total_cost = sum(item["totalCost"] for item in response.rows)

print(f"ğŸ’° Custo total nos Ãºltimos 30 dias: ${total_cost:.2f}")


# Verifica se o custo ultrapassou o limite
if total_cost > BUDGET_LIMIT:
    print("âš ï¸ ALERTA: O custo ultrapassou o limite definido!")
else:
    print("âœ… Tudo certo! O custo estÃ¡ dentro do orÃ§amento.")
```

Esse script **autentica no Azure**, **consulta os custos do Data Factory** e **gera alertas caso os gastos ultrapassem o limite definido**. ğŸš€

---

## ğŸ¯ Resultados Esperados

- âœ… Melhor controle dos custos no Azure ğŸ’°.
- âœ… ConfiguraÃ§Ã£o eficiente do **Data Factory** ğŸ—.
- âœ… Dashboards intuitivos para acompanhamento ğŸ“Š.
- âœ… AutomaÃ§Ã£o e infraestrutura como cÃ³digo ğŸ“œ.
- âœ… Monitoramento automatizado via **Python** ğŸ¤–.

---

## ğŸ“¢ ContribuiÃ§Ãµes

Sinta-se Ã  vontade para contribuir! SugestÃµes, melhorias e feedbacks sÃ£o bem-vindos.

ğŸ“© Para dÃºvidas ou sugestÃµes, [entre em contato](https://www.linkedin.com/in/devcaiada)!

> ğŸš€ **Vamos juntos monitorar e otimizar nosso uso no Azure!** ğŸ’™
----
<br></br>
<br></br>

# ğŸ›  RedundÃ¢ncia de Arquivos no Azure com Data Factory

## âœ¨ VisÃ£o Geral

Este projeto tem como objetivo criar um **processo completo de redundÃ¢ncia de arquivos** utilizando recursos do **Microsoft Azure**. AtravÃ©s do **Azure Data Factory**, vocÃª aprenderÃ¡ a configurar toda a infraestrutura necessÃ¡ria para mover dados entre ambientes **on-premises** e a **nuvem**, garantindo backup seguro e acessÃ­vel.

## ğŸ”„ Fluxo do Processo
1. **Conectar fontes de dados**: SQL Server local e Azure SQL Database.
2. **Criar Linked Services**: Estabelecendo conexÃ£o entre o Data Factory e os repositÃ³rios de dados.
3. **Criar Datasets**: DefiniÃ§Ã£o das estruturas para entrada e saÃ­da dos dados.
4. **Criar Pipelines**: ConstruÃ§Ã£o dos fluxos de trabalho para movimentaÃ§Ã£o dos dados.
5. **Converter e armazenar**: Transformar dados em **arquivos .TXT**, organizando-os em camadas (**raw/bronze**) dentro do **Azure Data Lake**.
6. **Publicar e Executar**: ValidaÃ§Ã£o e execuÃ§Ã£o do pipeline.
7. **Analisar performance**: Aplicando boas prÃ¡ticas para otimizar o processo.

---
## ğŸ› ï¸ Tecnologias Utilizadas

- **Microsoft Azure** (ğŸŒ Plataforma Cloud)
- **Azure Data Factory** (ğŸ›  OrquestraÃ§Ã£o de Dados)
- **SQL Server (On-Premises e Azure SQL Database)** (ğŸ’¾ Banco de Dados Relacional)
- **Azure Blob Storage** (ğŸ¢ Armazenamento de Arquivos)
- **Integration Runtime** (âš¡ Conectividade HÃ­brida)

---
## ğŸ—’ï¸ Passo a Passo da ImplementaÃ§Ã£o

### 1. Criando o Azure Data Factory
1. Acesse o portal do **Azure**.
2. Crie um novo **Data Factory**.
3. Configure o **Integration Runtime** para conectar-se ao SQL Server local.

### 2. Configurando os Linked Services
- Adicione um **Linked Service** para o SQL Server On-Premises.
- Adicione um **Linked Service** para o Azure SQL Database.
- Adicione um **Linked Service** para o Blob Storage.

### 3. Criando os Datasets
- Crie um **Dataset** apontando para a tabela SQL de origem.
- Crie um **Dataset** para os arquivos **.TXT** de destino no Blob Storage.

### 4. Criando o Pipeline
- Adicione um **Copy Activity** para mover os dados do SQL Server para o Azure Data Lake.
- Configure a transformaÃ§Ã£o dos dados em **arquivos .TXT** organizados por camadas (**raw/bronze**).
- Teste e valide as transferÃªncias.

### 5. Publicando e Executando
- Publique as alteraÃ§Ãµes e execute o pipeline.
- Monitore os logs e valide a performance.

---
## ğŸ’¡ Utilizando o SDK do Azure no Python
O **SDK do Azure para Python** permite interagir programaticamente com os serviÃ§os da nuvem, como o **Azure Blob Storage**. Com ele, vocÃª pode realizar operaÃ§Ãµes como upload, download e gerenciamento de arquivos de forma eficiente.

Para instalar:
```bash
pip install azure-storage-blob
```

### Exemplo de cÃ³digo para upload de arquivo no Azure Blob Storage
```python
from azure.storage.blob import BlobServiceClient

# ConfiguraÃ§Ã£o
connect_str = "<SUA_CONNECTION_STRING>"
container_name = "meu-container"
blob_name = "meu-arquivo.txt"
file_path = "./meu-arquivo.txt"

# Criar o cliente do serviÃ§o Blob
blob_service_client = BlobServiceClient.from_connection_string(connect_str)
blob_client = blob_service_client.get_blob_client(container=container_name, blob=blob_name)

# Upload do arquivo
with open(file_path, "rb") as data:
    blob_client.upload_blob(data)

print(f"Arquivo {blob_name} enviado com sucesso!")
```

Confira a [documentaÃ§Ã£o oficial](https://learn.microsoft.com/en-us/azure/storage/blobs/) para mais detalhes sobre suas funcionalidades.

---
## ğŸŒŸ BenefÃ­cios
- ğŸŒ **Alta disponibilidade**: Backup automÃ¡tico na nuvem.
- âš–ï¸ **SeguranÃ§a**: Dados armazenados de forma redundante.
- â³ **EficiÃªncia**: Pipelines otimizados para melhor desempenho.

---
## ğŸ’¼ ContribuiÃ§Ã£o
Fique Ã  vontade para **sugerir melhorias** ou abrir um **Pull Request**! Qualquer dÃºvida, me chamem! ğŸš€

---
## ğŸ“… LicenÃ§a
Este projeto estÃ¡ sob a **Unlicense**, permitindo seu uso e modificaÃ§Ã£o sem restriÃ§Ãµes.

---
ğŸ‘‰ **Vamos juntos dominar o Azure!** ğŸŒŸ



