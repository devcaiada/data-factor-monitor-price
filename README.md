# ğŸ“Š Criando um Monitoramento de Custos no Data Factory

## ğŸš€ VisÃ£o Geral

Este projeto apresenta uma abordagem prÃ¡tica para explorar o ambiente **Azure** utilizando uma conta gratuita de estudante. O objetivo principal Ã© **configurar o Azure Data Factory e monitorar o uso e os custos dos recursos implantados**. Durante a implementaÃ§Ã£o, abordaremos os seguintes temas:

- âœ”ï¸ EstruturaÃ§Ã£o de assinaturas e grupos de recursos.
- âœ”ï¸ Boas prÃ¡ticas de nomenclatura para facilitar a organizaÃ§Ã£o.
- âœ”ï¸ PersonalizaÃ§Ã£o de dashboards para acompanhamento visual.
- âœ”ï¸ UtilizaÃ§Ã£o de mÃ©tricas e alertas de custo para controle eficiente.
- âœ”ï¸ AutomaÃ§Ã£o com **ARM Templates** e **Azure Cloud Shell**.
- âœ”ï¸ Passo a passo completo para monitorar os gastos na plataforma.
- âœ”ï¸ ImplementaÃ§Ã£o de um **script Python** para automaÃ§Ã£o do monitoramento de custos.

---

## ğŸ› ï¸ Tecnologias Utilizadas

- **Azure Data Factory** ğŸŒ
- **Azure Monitor** ğŸ“Š
- **Azure Cost Management** ğŸ’°
- **Azure Cloud Shell** ğŸ–¥ï¸
- **ARM Templates** ğŸ“„
- **Python + Azure SDK** ğŸ

---

## ğŸ“Œ PrÃ©-requisitos

Antes de iniciar, certifique-se de ter:

- ğŸ”¹ Uma conta gratuita no **Azure for Students** ou **Azure Free Tier**.
- ğŸ”¹ Acesso ao **Azure Portal**.
- ğŸ”¹ Familiaridade com o **Azure Resource Manager (ARM)**.
- ğŸ”¹ Conhecimentos bÃ¡sicos em **PowerShell**, **CLI do Azure** e **Python**.

---

## ğŸ— Passo a Passo da ImplementaÃ§Ã£o

### 1ï¸âƒ£ Criando o Ambiente no Azure

1. Acesse o portal [Azure Portal](https://portal.azure.com/).
2. Crie um **Grupo de Recursos** para organizar os ativos.
3. Configure uma **Assinatura** vinculada Ã  conta gratuita.
4. Defina um **Plano de Nomenclatura** para manter a padronizaÃ§Ã£o.

### 2ï¸âƒ£ Implantando o Azure Data Factory

1. No **Azure Portal**, vÃ¡ atÃ© "Criar um recurso" e selecione **Data Factory**.
2. Escolha o grupo de recursos criado anteriormente.
3. Defina a regiÃ£o e nome do serviÃ§o seguindo as boas prÃ¡ticas.
4. Conclua a implantaÃ§Ã£o e acesse o painel do **Data Factory**.

### 3ï¸âƒ£ Monitorando os Custos

1. Acesse **Azure Cost Management + Billing** no portal.
2. Configure **mÃ©tricas de uso e custo** para o Data Factory.
3. Defina **alertas de custo** para evitar gastos inesperados.
4. Crie um **dashboard personalizado** para visualizaÃ§Ã£o simplificada.

### 4ï¸âƒ£ AutomaÃ§Ã£o com Azure Cloud Shell

1. Acesse o **Azure Cloud Shell** no portal.
2. Utilize **PowerShell** ou **Azure CLI** para gerenciar recursos.
3. Automatize tarefas recorrentes com **scripts ARM Templates**.
4. Teste a criaÃ§Ã£o e remoÃ§Ã£o automatizada de recursos para otimizaÃ§Ã£o.

### 5ï¸âƒ£ ImplementaÃ§Ã£o com Python ğŸš€

Podemos automatizar o monitoramento dos custos do **Azure Data Factory** utilizando Python e a **Azure SDK**. Siga os passos abaixo:

#### ğŸ“Œ Instale as bibliotecas necessÃ¡rias:
```sh
pip install azure-identity azure-mgmt-costmanagement
```

#### ğŸ“ CÃ³digo em Python para Monitoramento de Custos:
```python
from azure.identity import DefaultAzureCredential
from azure.mgmt.costmanagement import CostManagementClient
import datetime


SUBSCRIPTION_ID = "xxxx-xxxx-xxxx-xxxx"
BUDGET_LIMIT = 50


credential = DefaultAzureCredential()
client = CostManagementClient(credential)

# Define o perÃ­odo para consulta (Ãºltimos 30 dias)
end_date = datetime.date.today()
start_date = end_date - datetime.timedelta(days=30)

# Consulta os custos do Data Factory
cost_request = {
    "type": "Usage",
    "timeframe": "Custom",
    "time_period": {
        "from": start_date.strftime('%Y-%m-%d'),
        "to": end_date.strftime('%Y-%m-%d')
    },
    "dataset": {
        "granularity": "Daily",
        "aggregation": {
            "totalCost": {
                "name": "PreTaxCost",
                "function": "Sum"
            }
        }
    }
}


response = client.query.usage(f"/subscriptions/{SUBSCRIPTION_ID}", cost_request)
total_cost = sum(item["totalCost"] for item in response.rows)

print(f"ğŸ’° Custo total nos Ãºltimos 30 dias: ${total_cost:.2f}")


# Verifica se o custo ultrapassou o limite
if total_cost > BUDGET_LIMIT:
    print("âš ï¸ ALERTA: O custo ultrapassou o limite definido!")
else:
    print("âœ… Tudo certo! O custo estÃ¡ dentro do orÃ§amento.")
```

Esse script **autentica no Azure**, **consulta os custos do Data Factory** e **gera alertas caso os gastos ultrapassem o limite definido**. ğŸš€

---

## ğŸ¯ Resultados Esperados

- âœ… Melhor controle dos custos no Azure ğŸ’°.
- âœ… ConfiguraÃ§Ã£o eficiente do **Data Factory** ğŸ—.
- âœ… Dashboards intuitivos para acompanhamento ğŸ“Š.
- âœ… AutomaÃ§Ã£o e infraestrutura como cÃ³digo ğŸ“œ.
- âœ… Monitoramento automatizado via **Python** ğŸ¤–.

---

## ğŸ“¢ ContribuiÃ§Ãµes

Sinta-se Ã  vontade para contribuir! SugestÃµes, melhorias e feedbacks sÃ£o bem-vindos.

ğŸ“© Para dÃºvidas ou sugestÃµes, [entre em contato](https://www.linkedin.com/in/devcaiada)!

> ğŸš€ **Vamos juntos monitorar e otimizar nosso uso no Azure!** ğŸ’™
----
<br></br>
<br></br>

# ğŸ›  RedundÃ¢ncia de Arquivos no Azure com Data Factory

## âœ¨ VisÃ£o Geral

Este projeto tem como objetivo criar um **processo completo de redundÃ¢ncia de arquivos** utilizando recursos do **Microsoft Azure**. AtravÃ©s do **Azure Data Factory**, vocÃª aprenderÃ¡ a configurar toda a infraestrutura necessÃ¡ria para mover dados entre ambientes **on-premises** e a **nuvem**, garantindo backup seguro e acessÃ­vel.

## ğŸ”„ Fluxo do Processo
1. **Conectar fontes de dados**: SQL Server local e Azure SQL Database.
2. **Criar Linked Services**: Estabelecendo conexÃ£o entre o Data Factory e os repositÃ³rios de dados.
3. **Criar Datasets**: DefiniÃ§Ã£o das estruturas para entrada e saÃ­da dos dados.
4. **Criar Pipelines**: ConstruÃ§Ã£o dos fluxos de trabalho para movimentaÃ§Ã£o dos dados.
5. **Converter e armazenar**: Transformar dados em **arquivos .TXT**, organizando-os em camadas (**raw/bronze**) dentro do **Azure Data Lake**.
6. **Publicar e Executar**: ValidaÃ§Ã£o e execuÃ§Ã£o do pipeline.
7. **Analisar performance**: Aplicando boas prÃ¡ticas para otimizar o processo.

---
## ğŸ› ï¸ Tecnologias Utilizadas

- **Microsoft Azure** (ğŸŒ Plataforma Cloud)
- **Azure Data Factory** (ğŸ›  OrquestraÃ§Ã£o de Dados)
- **SQL Server (On-Premises e Azure SQL Database)** (ğŸ’¾ Banco de Dados Relacional)
- **Azure Blob Storage** (ğŸ¢ Armazenamento de Arquivos)
- **Integration Runtime** (âš¡ Conectividade HÃ­brida)

---
## ğŸ—’ï¸ Passo a Passo da ImplementaÃ§Ã£o

### 1. Criando o Azure Data Factory
1. Acesse o portal do **Azure**.
2. Crie um novo **Data Factory**.
3. Configure o **Integration Runtime** para conectar-se ao SQL Server local.

### 2. Configurando os Linked Services
- Adicione um **Linked Service** para o SQL Server On-Premises.
- Adicione um **Linked Service** para o Azure SQL Database.
- Adicione um **Linked Service** para o Blob Storage.

### 3. Criando os Datasets
- Crie um **Dataset** apontando para a tabela SQL de origem.
- Crie um **Dataset** para os arquivos **.TXT** de destino no Blob Storage.

### 4. Criando o Pipeline
- Adicione um **Copy Activity** para mover os dados do SQL Server para o Azure Data Lake.
- Configure a transformaÃ§Ã£o dos dados em **arquivos .TXT** organizados por camadas (**raw/bronze**).
- Teste e valide as transferÃªncias.

### 5. Publicando e Executando
- Publique as alteraÃ§Ãµes e execute o pipeline.
- Monitore os logs e valide a performance.

---
## ğŸ’¡ Utilizando o SDK do Azure no Python
O **SDK do Azure para Python** permite interagir programaticamente com os serviÃ§os da nuvem, como o **Azure Blob Storage**. Com ele, vocÃª pode realizar operaÃ§Ãµes como upload, download e gerenciamento de arquivos de forma eficiente.

Para instalar:
```bash
pip install azure-storage-blob
```

### Exemplo de cÃ³digo para upload de arquivo no Azure Blob Storage
```python
from azure.storage.blob import BlobServiceClient

# ConfiguraÃ§Ã£o
connect_str = "<SUA_CONNECTION_STRING>"
container_name = "meu-container"
blob_name = "meu-arquivo.txt"
file_path = "./meu-arquivo.txt"

# Criar o cliente do serviÃ§o Blob
blob_service_client = BlobServiceClient.from_connection_string(connect_str)
blob_client = blob_service_client.get_blob_client(container=container_name, blob=blob_name)

# Upload do arquivo
with open(file_path, "rb") as data:
    blob_client.upload_blob(data)

print(f"Arquivo {blob_name} enviado com sucesso!")
```

Confira a [documentaÃ§Ã£o oficial](https://learn.microsoft.com/en-us/azure/storage/blobs/) para mais detalhes sobre suas funcionalidades.

---
## ğŸŒŸ BenefÃ­cios
- ğŸŒ **Alta disponibilidade**: Backup automÃ¡tico na nuvem.
- âš–ï¸ **SeguranÃ§a**: Dados armazenados de forma redundante.
- â³ **EficiÃªncia**: Pipelines otimizados para melhor desempenho.

---
## ğŸ’¼ ContribuiÃ§Ã£o
Fique Ã  vontade para **sugerir melhorias** ou abrir um **Pull Request**! Qualquer dÃºvida, me chamem! ğŸš€

---
## ğŸ“… LicenÃ§a
Este projeto estÃ¡ sob a **Unlicense**, permitindo seu uso e modificaÃ§Ã£o sem restriÃ§Ãµes.

---
ğŸ‘‰ **Vamos juntos dominar o Azure!** ğŸŒŸ

-----

<br></br>
<br></br>

# Azure Databricks - Versionamento e OrganizaÃ§Ã£o de Notebooks

## DescriÃ§Ã£o
Este projeto demonstra como utilizar o **Azure Databricks** para versionamento e organizaÃ§Ã£o de notebooks em ambientes de dados. A proposta inclui:

- CriaÃ§Ã£o e configuraÃ§Ã£o de **clusters**;
- ImportaÃ§Ã£o e execuÃ§Ã£o de **notebooks** com suporte de **InteligÃªncia Artificial**;
- IntegraÃ§Ã£o com **Azure DevOps** para controle de cÃ³digo e automaÃ§Ã£o de pipelines de **CI/CD**;
- Uso da IA integrada ao Databricks para geraÃ§Ã£o de cÃ³digo **Python e Spark**;
- Boas prÃ¡ticas para organizaÃ§Ã£o, exportaÃ§Ã£o e reaproveitamento de notebooks;
- ExploraÃ§Ã£o de recursos do **Microsoft Learn**, com exercÃ­cios guiados e roteiros de aprendizado;
- Trabalho colaborativo e seguro com versionamento estruturado em **engenharia de dados e machine learning**.

---
## Arquitetura

A arquitetura deste projeto segue o fluxo abaixo:

1. **CriaÃ§Ã£o de um Cluster** no Azure Databricks.
2. **ImportaÃ§Ã£o de arquivos e notebooks** para execuÃ§Ã£o.
3. **ExecuÃ§Ã£o de notebooks interativos** com filtros, sumarizaÃ§Ãµes e visualizaÃ§Ãµes.
4. **GeraÃ§Ã£o de cÃ³digo com suporte de IA** integrada ao Databricks.
5. **IntegraÃ§Ã£o com Azure DevOps** para versionamento e CI/CD.
6. **AutomaÃ§Ã£o de pipelines** para controle das execuÃ§Ãµes e governanÃ§a.

![Arquitetura Azure Databricks](https://www.databricks.com/sites/default/files/2023-03/azure-azure-databricks-img.png?v=1678449355)

---
## Tecnologias Utilizadas

- **Azure Databricks**
- **Python**
- **Apache Spark**
- **Azure DevOps**
- **Microsoft Learn**
- **CI/CD (Continuous Integration & Continuous Deployment)**
- **MLflow (para versionamento de modelos em ML)**

---
## Passo a Passo - ConfiguraÃ§Ã£o do Ambiente

### 1. Criar um Cluster no Databricks
1. Acesse [Azure Databricks](https://portal.azure.com/)
2. Navegue atÃ© `Clusters > Create Cluster`
3. Escolha um nome e selecione a configuraÃ§Ã£o de hardware necessÃ¡ria
4. Clique em `Create Cluster`

### 2. Importar e Executar um Notebook
1. No Databricks, acesse `Workspace`
2. Clique em `Import` e carregue um arquivo `.ipynb` ou `.dbc`
3. Abra o notebook para ediÃ§Ã£o e execuÃ§Ã£o

### 3. Configurar Azure DevOps para Versionamento
1. No Azure DevOps, crie um repositÃ³rio `Git`
2. Conecte o Databricks ao Azure DevOps: 
   - VÃ¡ para `Repos > Git Integration`
   - Configure a conexÃ£o ao seu repositÃ³rio remoto
3. Habilite `CI/CD Pipelines` para automaÃ§Ã£o

### 4. Criar um Pipeline CI/CD no Azure DevOps
1. No Azure DevOps, vÃ¡ para `Pipelines > New Pipeline`
2. Escolha `GitHub` ou `Azure Repos Git` como origem do cÃ³digo
3. Selecione `Starter Pipeline` e edite o `azure-pipelines.yml`
4. Adicione o seguinte cÃ³digo para execuÃ§Ã£o automatizada de notebooks:

```yaml
trigger:
  branches:
    include:
      - main

pool:
  vmImage: 'ubuntu-latest'

steps:
- task: DatabricksRunNotebook@0
  inputs:
    databricksServiceConnection: 'AzureDatabricks'
    notebookPath: '/Workspace/MeuNotebook'
    workspaceUrl: 'https://adb-123456789.azuredatabricks.net'
    newCluster:
      clusterName: 'ci-cd-cluster'
      nodeTypeId: 'Standard_DS3_v2'
      sparkVersion: '7.3.x-scala2.12'
```

5. Salve e execute o pipeline para testar a automaÃ§Ã£o

---
## Exemplo de CÃ³digo em Python e Spark

Abaixo, um exemplo de cÃ³digo para leitura, transformaÃ§Ã£o e exibiÃ§Ã£o de dados em um notebook do Databricks:

```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, sum

# Criar sessÃ£o Spark
spark = SparkSession.builder.appName("DatabricksExample").getOrCreate()

# Carregar dataset (exemplo fictÃ­cio)
data = [("Produto A", 1000), ("Produto B", 1500), ("Produto C", 700)]
df = spark.createDataFrame(data, ["Produto", "Vendas"])

# TransformaÃ§Ã£o: sumarizar vendas
df_sum = df.groupBy("Produto").agg(sum(col("Vendas")).alias("Total_Vendas"))

# Exibir resultado
df_sum.show()
```

---
## BenefÃ­cios do Projeto

âœ… **Melhor organizaÃ§Ã£o e versionamento de notebooks**
âœ… **AutomaÃ§Ã£o de processos com CI/CD**
âœ… **ColaboraÃ§Ã£o eficiente em times de engenharia e ciÃªncia de dados**
âœ… **Uso de InteligÃªncia Artificial para facilitar o desenvolvimento**
âœ… **GovernanÃ§a e seguranÃ§a no controle de cÃ³digo**

---
## Recursos Extras

- [DocumentaÃ§Ã£o Oficial do Azure Databricks](https://learn.microsoft.com/en-us/azure/databricks/)
- [IntroduÃ§Ã£o ao Apache Spark](https://spark.apache.org/)
- [ConfiguraÃ§Ã£o de RepositÃ³rios no Databricks](https://learn.microsoft.com/en-us/azure/databricks/repos/)
- [GitHub Actions para Azure Databricks](https://github.com/marketplace/actions/databricks-run-notebook)

---
## ContribuiÃ§Ãµes

Fique Ã  vontade para contribuir! Caso tenha sugestÃµes, **abra uma issue ou envie um pull request**. ğŸš€


