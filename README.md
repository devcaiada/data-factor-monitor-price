# üìä Criando um Monitoramento de Custos no Data Factory

## üöÄ Vis√£o Geral

Este projeto apresenta uma abordagem pr√°tica para explorar o ambiente **Azure** utilizando uma conta gratuita de estudante. O objetivo principal √© **configurar o Azure Data Factory e monitorar o uso e os custos dos recursos implantados**. Durante a implementa√ß√£o, abordaremos os seguintes temas:

- ‚úîÔ∏è Estrutura√ß√£o de assinaturas e grupos de recursos.
- ‚úîÔ∏è Boas pr√°ticas de nomenclatura para facilitar a organiza√ß√£o.
- ‚úîÔ∏è Personaliza√ß√£o de dashboards para acompanhamento visual.
- ‚úîÔ∏è Utiliza√ß√£o de m√©tricas e alertas de custo para controle eficiente.
- ‚úîÔ∏è Automa√ß√£o com **ARM Templates** e **Azure Cloud Shell**.
- ‚úîÔ∏è Passo a passo completo para monitorar os gastos na plataforma.
- ‚úîÔ∏è Implementa√ß√£o de um **script Python** para automa√ß√£o do monitoramento de custos.

---

## üõ†Ô∏è Tecnologias Utilizadas

- **Azure Data Factory** üåê
- **Azure Monitor** üìä
- **Azure Cost Management** üí∞
- **Azure Cloud Shell** üñ•Ô∏è
- **ARM Templates** üìÑ
- **Python + Azure SDK** üêç

---

## üìå Pr√©-requisitos

Antes de iniciar, certifique-se de ter:

- üîπ Uma conta gratuita no **Azure for Students** ou **Azure Free Tier**.
- üîπ Acesso ao **Azure Portal**.
- üîπ Familiaridade com o **Azure Resource Manager (ARM)**.
- üîπ Conhecimentos b√°sicos em **PowerShell**, **CLI do Azure** e **Python**.

---

## üèó Passo a Passo da Implementa√ß√£o

### 1Ô∏è‚É£ Criando o Ambiente no Azure

1. Acesse o portal [Azure Portal](https://portal.azure.com/).
2. Crie um **Grupo de Recursos** para organizar os ativos.
3. Configure uma **Assinatura** vinculada √† conta gratuita.
4. Defina um **Plano de Nomenclatura** para manter a padroniza√ß√£o.

### 2Ô∏è‚É£ Implantando o Azure Data Factory

1. No **Azure Portal**, v√° at√© "Criar um recurso" e selecione **Data Factory**.
2. Escolha o grupo de recursos criado anteriormente.
3. Defina a regi√£o e nome do servi√ßo seguindo as boas pr√°ticas.
4. Conclua a implanta√ß√£o e acesse o painel do **Data Factory**.

### 3Ô∏è‚É£ Monitorando os Custos

1. Acesse **Azure Cost Management + Billing** no portal.
2. Configure **m√©tricas de uso e custo** para o Data Factory.
3. Defina **alertas de custo** para evitar gastos inesperados.
4. Crie um **dashboard personalizado** para visualiza√ß√£o simplificada.

### 4Ô∏è‚É£ Automa√ß√£o com Azure Cloud Shell

1. Acesse o **Azure Cloud Shell** no portal.
2. Utilize **PowerShell** ou **Azure CLI** para gerenciar recursos.
3. Automatize tarefas recorrentes com **scripts ARM Templates**.
4. Teste a cria√ß√£o e remo√ß√£o automatizada de recursos para otimiza√ß√£o.

### 5Ô∏è‚É£ Implementa√ß√£o com Python üöÄ

Podemos automatizar o monitoramento dos custos do **Azure Data Factory** utilizando Python e a **Azure SDK**. Siga os passos abaixo:

#### üìå Instale as bibliotecas necess√°rias:
```sh
pip install azure-identity azure-mgmt-costmanagement
```

#### üìù C√≥digo em Python para Monitoramento de Custos:
```python
from azure.identity import DefaultAzureCredential
from azure.mgmt.costmanagement import CostManagementClient
import datetime


SUBSCRIPTION_ID = "xxxx-xxxx-xxxx-xxxx"
BUDGET_LIMIT = 50


credential = DefaultAzureCredential()
client = CostManagementClient(credential)

# Define o per√≠odo para consulta (√∫ltimos 30 dias)
end_date = datetime.date.today()
start_date = end_date - datetime.timedelta(days=30)

# Consulta os custos do Data Factory
cost_request = {
    "type": "Usage",
    "timeframe": "Custom",
    "time_period": {
        "from": start_date.strftime('%Y-%m-%d'),
        "to": end_date.strftime('%Y-%m-%d')
    },
    "dataset": {
        "granularity": "Daily",
        "aggregation": {
            "totalCost": {
                "name": "PreTaxCost",
                "function": "Sum"
            }
        }
    }
}


response = client.query.usage(f"/subscriptions/{SUBSCRIPTION_ID}", cost_request)
total_cost = sum(item["totalCost"] for item in response.rows)

print(f"üí∞ Custo total nos √∫ltimos 30 dias: ${total_cost:.2f}")


# Verifica se o custo ultrapassou o limite
if total_cost > BUDGET_LIMIT:
    print("‚ö†Ô∏è ALERTA: O custo ultrapassou o limite definido!")
else:
    print("‚úÖ Tudo certo! O custo est√° dentro do or√ßamento.")
```

Esse script **autentica no Azure**, **consulta os custos do Data Factory** e **gera alertas caso os gastos ultrapassem o limite definido**. üöÄ

---

## üéØ Resultados Esperados

- ‚úÖ Melhor controle dos custos no Azure üí∞.
- ‚úÖ Configura√ß√£o eficiente do **Data Factory** üèó.
- ‚úÖ Dashboards intuitivos para acompanhamento üìä.
- ‚úÖ Automa√ß√£o e infraestrutura como c√≥digo üìú.
- ‚úÖ Monitoramento automatizado via **Python** ü§ñ.

---

## üì¢ Contribui√ß√µes

Sinta-se √† vontade para contribuir! Sugest√µes, melhorias e feedbacks s√£o bem-vindos.

üì© Para d√∫vidas ou sugest√µes, [entre em contato](https://www.linkedin.com/in/devcaiada)!

> üöÄ **Vamos juntos monitorar e otimizar nosso uso no Azure!** üíô
----
<br></br>
<br></br>

# üõ† Redund√¢ncia de Arquivos no Azure com Data Factory

## ‚ú® Vis√£o Geral

Este projeto tem como objetivo criar um **processo completo de redund√¢ncia de arquivos** utilizando recursos do **Microsoft Azure**. Atrav√©s do **Azure Data Factory**, voc√™ aprender√° a configurar toda a infraestrutura necess√°ria para mover dados entre ambientes **on-premises** e a **nuvem**, garantindo backup seguro e acess√≠vel.

## üîÑ Fluxo do Processo
1. **Conectar fontes de dados**: SQL Server local e Azure SQL Database.
2. **Criar Linked Services**: Estabelecendo conex√£o entre o Data Factory e os reposit√≥rios de dados.
3. **Criar Datasets**: Defini√ß√£o das estruturas para entrada e sa√≠da dos dados.
4. **Criar Pipelines**: Constru√ß√£o dos fluxos de trabalho para movimenta√ß√£o dos dados.
5. **Converter e armazenar**: Transformar dados em **arquivos .TXT**, organizando-os em camadas (**raw/bronze**) dentro do **Azure Data Lake**.
6. **Publicar e Executar**: Valida√ß√£o e execu√ß√£o do pipeline.
7. **Analisar performance**: Aplicando boas pr√°ticas para otimizar o processo.

---
## üõ†Ô∏è Tecnologias Utilizadas

- **Microsoft Azure** (üåç Plataforma Cloud)
- **Azure Data Factory** (üõ† Orquestra√ß√£o de Dados)
- **SQL Server (On-Premises e Azure SQL Database)** (üíæ Banco de Dados Relacional)
- **Azure Blob Storage** (üè¢ Armazenamento de Arquivos)
- **Integration Runtime** (‚ö° Conectividade H√≠brida)

---
## üóíÔ∏è Passo a Passo da Implementa√ß√£o

### 1. Criando o Azure Data Factory
1. Acesse o portal do **Azure**.
2. Crie um novo **Data Factory**.
3. Configure o **Integration Runtime** para conectar-se ao SQL Server local.

### 2. Configurando os Linked Services
- Adicione um **Linked Service** para o SQL Server On-Premises.
- Adicione um **Linked Service** para o Azure SQL Database.
- Adicione um **Linked Service** para o Blob Storage.

### 3. Criando os Datasets
- Crie um **Dataset** apontando para a tabela SQL de origem.
- Crie um **Dataset** para os arquivos **.TXT** de destino no Blob Storage.

### 4. Criando o Pipeline
- Adicione um **Copy Activity** para mover os dados do SQL Server para o Azure Data Lake.
- Configure a transforma√ß√£o dos dados em **arquivos .TXT** organizados por camadas (**raw/bronze**).
- Teste e valide as transfer√™ncias.

### 5. Publicando e Executando
- Publique as altera√ß√µes e execute o pipeline.
- Monitore os logs e valide a performance.

---
## üí° Utilizando o SDK do Azure no Python
O **SDK do Azure para Python** permite interagir programaticamente com os servi√ßos da nuvem, como o **Azure Blob Storage**. Com ele, voc√™ pode realizar opera√ß√µes como upload, download e gerenciamento de arquivos de forma eficiente.

Para instalar:
```bash
pip install azure-storage-blob
```

### Exemplo de c√≥digo para upload de arquivo no Azure Blob Storage
```python
from azure.storage.blob import BlobServiceClient

# Configura√ß√£o
connect_str = "<SUA_CONNECTION_STRING>"
container_name = "meu-container"
blob_name = "meu-arquivo.txt"
file_path = "./meu-arquivo.txt"

# Criar o cliente do servi√ßo Blob
blob_service_client = BlobServiceClient.from_connection_string(connect_str)
blob_client = blob_service_client.get_blob_client(container=container_name, blob=blob_name)

# Upload do arquivo
with open(file_path, "rb") as data:
    blob_client.upload_blob(data)

print(f"Arquivo {blob_name} enviado com sucesso!")
```

Confira a [documenta√ß√£o oficial](https://learn.microsoft.com/en-us/azure/storage/blobs/) para mais detalhes sobre suas funcionalidades.

---
## üåü Benef√≠cios
- üåê **Alta disponibilidade**: Backup autom√°tico na nuvem.
- ‚öñÔ∏è **Seguran√ßa**: Dados armazenados de forma redundante.
- ‚è≥ **Efici√™ncia**: Pipelines otimizados para melhor desempenho.

---
## üíº Contribui√ß√£o
Fique √† vontade para **sugerir melhorias** ou abrir um **Pull Request**! Qualquer d√∫vida, me chamem! üöÄ

---
## üìÖ Licen√ßa
Este projeto est√° sob a **Unlicense**, permitindo seu uso e modifica√ß√£o sem restri√ß√µes.

---
üëâ **Vamos juntos dominar o Azure!** üåü

-----

<br></br>
<br></br>

# Azure Databricks - Versionamento e Organiza√ß√£o de Notebooks

## Descri√ß√£o
Este projeto demonstra como utilizar o **Azure Databricks** para versionamento e organiza√ß√£o de notebooks em ambientes de dados. A proposta inclui:

- Cria√ß√£o e configura√ß√£o de **clusters**;
- Importa√ß√£o e execu√ß√£o de **notebooks** com suporte de **Intelig√™ncia Artificial**;
- Integra√ß√£o com **Azure DevOps** para controle de c√≥digo e automa√ß√£o de pipelines de **CI/CD**;
- Uso da IA integrada ao Databricks para gera√ß√£o de c√≥digo **Python e Spark**;
- Boas pr√°ticas para organiza√ß√£o, exporta√ß√£o e reaproveitamento de notebooks;
- Explora√ß√£o de recursos do **Microsoft Learn**, com exerc√≠cios guiados e roteiros de aprendizado;
- Trabalho colaborativo e seguro com versionamento estruturado em **engenharia de dados e machine learning**.

---
## Arquitetura

A arquitetura deste projeto segue o fluxo abaixo:

1. **Cria√ß√£o de um Cluster** no Azure Databricks.
2. **Importa√ß√£o de arquivos e notebooks** para execu√ß√£o.
3. **Execu√ß√£o de notebooks interativos** com filtros, sumariza√ß√µes e visualiza√ß√µes.
4. **Gera√ß√£o de c√≥digo com suporte de IA** integrada ao Databricks.
5. **Integra√ß√£o com Azure DevOps** para versionamento e CI/CD.
6. **Automa√ß√£o de pipelines** para controle das execu√ß√µes e governan√ßa.

![Arquitetura Azure Databricks](https://www.databricks.com/sites/default/files/2023-03/azure-azure-databricks-img.png?v=1678449355)

---
## Tecnologias Utilizadas

- **Azure Databricks**
- **Python**
- **Apache Spark**
- **Azure DevOps**
- **Microsoft Learn**
- **CI/CD (Continuous Integration & Continuous Deployment)**
- **MLflow (para versionamento de modelos em ML)**

---
## Passo a Passo - Configura√ß√£o do Ambiente

### 1. Criar um Cluster no Databricks
1. Acesse [Azure Databricks](https://portal.azure.com/)
2. Navegue at√© `Clusters > Create Cluster`
3. Escolha um nome e selecione a configura√ß√£o de hardware necess√°ria
4. Clique em `Create Cluster`

### 2. Importar e Executar um Notebook
1. No Databricks, acesse `Workspace`
2. Clique em `Import` e carregue um arquivo `.ipynb` ou `.dbc`
3. Abra o notebook para edi√ß√£o e execu√ß√£o

### 3. Configurar Azure DevOps para Versionamento
1. No Azure DevOps, crie um reposit√≥rio `Git`
2. Conecte o Databricks ao Azure DevOps: 
   - V√° para `Repos > Git Integration`
   - Configure a conex√£o ao seu reposit√≥rio remoto
3. Habilite `CI/CD Pipelines` para automa√ß√£o

### 4. Criar um Pipeline CI/CD no Azure DevOps
1. No Azure DevOps, v√° para `Pipelines > New Pipeline`
2. Escolha `GitHub` ou `Azure Repos Git` como origem do c√≥digo
3. Selecione `Starter Pipeline` e edite o `azure-pipelines.yml`
4. Adicione o seguinte c√≥digo para execu√ß√£o automatizada de notebooks:

```yaml
trigger:
  branches:
    include:
      - main

pool:
  vmImage: 'ubuntu-latest'

steps:
- task: DatabricksRunNotebook@0
  inputs:
    databricksServiceConnection: 'AzureDatabricks'
    notebookPath: '/Workspace/MeuNotebook'
    workspaceUrl: 'https://adb-123456789.azuredatabricks.net'
    newCluster:
      clusterName: 'ci-cd-cluster'
      nodeTypeId: 'Standard_DS3_v2'
      sparkVersion: '7.3.x-scala2.12'
```

5. Salve e execute o pipeline para testar a automa√ß√£o

---
## Exemplo de C√≥digo em Python e Spark

Abaixo, um exemplo de c√≥digo para leitura, transforma√ß√£o e exibi√ß√£o de dados em um notebook do Databricks:

```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, sum

# Criar sess√£o Spark
spark = SparkSession.builder.appName("DatabricksExample").getOrCreate()

# Carregar dataset (exemplo fict√≠cio)
data = [("Produto A", 1000), ("Produto B", 1500), ("Produto C", 700)]
df = spark.createDataFrame(data, ["Produto", "Vendas"])

# Transforma√ß√£o: sumarizar vendas
df_sum = df.groupBy("Produto").agg(sum(col("Vendas")).alias("Total_Vendas"))

# Exibir resultado
df_sum.show()
```

---
## Benef√≠cios do Projeto

‚úÖ **Melhor organiza√ß√£o e versionamento de notebooks**
‚úÖ **Automa√ß√£o de processos com CI/CD**
‚úÖ **Colabora√ß√£o eficiente em times de engenharia e ci√™ncia de dados**
‚úÖ **Uso de Intelig√™ncia Artificial para facilitar o desenvolvimento**
‚úÖ **Governan√ßa e seguran√ßa no controle de c√≥digo**

---
## Recursos Extras

- [Documenta√ß√£o Oficial do Azure Databricks](https://learn.microsoft.com/en-us/azure/databricks/)
- [Introdu√ß√£o ao Apache Spark](https://spark.apache.org/)
- [Configura√ß√£o de Reposit√≥rios no Databricks](https://learn.microsoft.com/en-us/azure/databricks/repos/)
- [GitHub Actions para Azure Databricks](https://github.com/marketplace/actions/databricks-run-notebook)

---
## Contribui√ß√µes

Fique √† vontade para contribuir! Caso tenha sugest√µes, **abra uma issue ou envie um pull request**. üöÄ

<br></br>
<br></br>

# Integrando o Azure Data Factory ao Azure DevOps

## Vis√£o Geral
Este projeto demonstra como integrar o **Azure Data Factory** ao **Azure DevOps**, permitindo **versionamento**, **controle de mudan√ßas** e **backups autom√°ticos** de pipelines e artefatos de dados. Essa integra√ß√£o assegura maior **governan√ßa** e **rastreabilidade** no desenvolvimento de solu√ß√µes de dados.

## Benef√≠cios da Integra√ß√£o
- **Versionamento de Pipelines**: Manuten√ß√£o de hist√≥rico de altera√ß√µes.
- **Controle de Mudan√ßas**: Padroniza√ß√£o e rastreamento de ajustes.
- **Backup Autom√°tico**: Seguran√ßa contra perda de artefatos.
- **Prepara√ß√£o para CI/CD**: Facilidade na automa√ß√£o de deploys futuros.

## Passo a Passo da Integra√ß√£o

### 1. Criar uma Organiza√ß√£o e um Projeto no Azure DevOps
1. Acesse o [Azure DevOps](https://dev.azure.com/).
2. Clique em **Criar nova organiza√ß√£o** (caso n√£o tenha uma).
3. Dentro da organiza√ß√£o, clique em **Novo projeto**.
4. Defina um nome para o projeto e escolha a visibilidade (*Privado* ou *P√∫blico*).
5. Clique em **Criar**.

### 2. Configurar o Reposit√≥rio Git
1. No projeto criado, v√° para **Reposit√≥rios**.
2. Escolha **Git** como tipo de reposit√≥rio.
3. Copie a URL do reposit√≥rio remoto para uso posterior.

### 3. Criar o Azure Data Factory
1. Acesse o [Portal do Azure](https://portal.azure.com/).
2. V√° at√© **Data Factory** e clique em **Criar**.
3. Preencha as informa√ß√µes do recurso (Nome, Regi√£o, Grupo de Recursos).
4. Em **Git Configuration**, selecione **Configure Git later** se quiser configurar posteriormente.
5. Clique em **Criar**.

### 4. Configurar a Integra√ß√£o com o Git
1. Acesse o **Azure Data Factory Studio**.
2. No canto superior direito, clique em **Manage**.
3. V√° para a aba **Git Configuration**.
4. Clique em **Configure** e insira:
   - **Provider**: Azure DevOps Git
   - **Account Name**: Nome da organiza√ß√£o no DevOps
   - **Project Name**: Nome do projeto criado
   - **Repository Name**: Nome do reposit√≥rio
   - **Branch**: Defina a branch padr√£o (*main* ou *develop*)
   - **Root Folder**: `/`
5. Clique em **Apply**.

### 5. Gerenciando os Artefatos no Git
1. Todos os pipelines e datasets criados ser√£o salvos no reposit√≥rio.
2. Para visualizar no Azure DevOps:
   - Acesse **Reposit√≥rios** no DevOps e veja os arquivos versionados.
   - Cada altera√ß√£o ser√° armazenada com um commit.
3. Para realizar mudan√ßas:
   - Fa√ßa edi√ß√µes no **Data Factory** e publique as altera√ß√µes.
   - No DevOps, crie **Pull Requests** para revisar e aprovar mudan√ßas antes do merge.

### 6. Boas Pr√°ticas de Governan√ßa
- **Utilizar branches dedicadas**: `feature/nova_funcionalidade`, `hotfix/corre√ß√£o`.
- **Implementar revis√µes de c√≥digo** com Pull Requests.
- **Manter padr√µes de nomenclatura** para pipelines e datasets.
- **Automatizar deploys futuros** com CI/CD utilizando Azure Pipelines.

## Conclus√£o
A integra√ß√£o do Azure Data Factory com o Azure DevOps proporciona maior governan√ßa, versionamento e controle sobre os pipelines de dados. Essa estrutura possibilita padronizar ambientes de desenvolvimento e criar uma esteira eficiente para automa√ß√£o futura de deploys no Azure.

---
### üìå Pr√≥ximos Passos
- Implementar uma esteira de **CI/CD** para automa√ß√£o do deploy.
- Criar **testes automatizados** para validar pipelines antes da publica√ß√£o.
- Configurar **pol√≠ticas de aprova√ß√£o** para garantir qualidade nas mudan√ßas.

üí° **Gostou do projeto? Contribua e compartilhe!** üöÄ
